Time: 2019-08-01 00:03:19.843689
model_name: UNet
train_df_name: train.csv
images_folder: /media/ags/DATA/CODE/kaggle/pneumothorax/data/train_png
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 32, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:04:00.635569
model_name: UNet
train_df_name: train.csv
images_folder: /media/ags/DATA/CODE/kaggle/pneumothorax/data/train_png
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 32, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:05:23.030821
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 32, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:08:03.528713
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 32, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:08:21.637399
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 32, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:09:12.634459
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 32, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:09:54.586396
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 32, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:10:28.085501
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 32, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:11:09.692310
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 32, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:11:34.535830
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 32, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:12:17.735525
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 32, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:14:24.559176
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 8, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:14:47.642495
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 8, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:15:43.090726
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 16, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:16:42.568182
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 16, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:19:02.823012
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 16, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:20:06.205572
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 16, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:23:32.621807
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 16, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:24:28.111262
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 16, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:26:03.006439
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 16, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:28:34.805230
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 16, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:30:05.455738
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 16, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:35:38.442790
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 32, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:36:08.679059
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 8, 'val': 4}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 00:37:11.679237
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 16, 'val': 8}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
