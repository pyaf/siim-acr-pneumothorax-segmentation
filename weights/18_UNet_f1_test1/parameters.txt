Time: 2019-08-01 12:15:08.532853
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test1
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 4, 'val': 8}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 12:16:26.405975
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test1
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 4, 'val': 8}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 12:17:42.412933
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test1
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 4, 'val': 8}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
Time: 2019-08-01 12:22:42.022762
model_name: UNet
train_df_name: train.csv
resume: False
pretrained: False
pretrained_path: weights//ckpt31.pth
folder: weights/18_UNet_f1_test1
fold: 1
total_folds: 7
num_samples: None
sampling class weights: None
size: 224
top_lr: 0.0005
base_lr: None
num_workers: 12
batchsize: {'train': 2, 'val': 2}
momentum: 0.95
mean: (0, 0, 0)
std: (1, 1, 1)
start_epoch: 0
augmentations: [Transpose(always_apply=False, p=0.5), Flip(always_apply=False, p=0.5), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0, 0), scale_limit=(-0.09999999999999998, 0.10000000000000009), rotate_limit=(-120, 120), interpolation=1, border_mode=0, value=None), ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None)]
criterion: BCEWithLogitsLoss()
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
remark: 
